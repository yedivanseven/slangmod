from typing import Any
from abc import ABC, abstractmethod
from functools import cached_property
import torch as pt
from swak.pt.types import Module, Tensor, Tensors2T
from ..tokenizers import Algo

__all__ = [
    'Generator',
    'NextToken'
]

type Logits = tuple[Tensor, int]


class Generator(ABC):
    """Abstract base class for text generators to inherit from.

    Parameters
    ----------
    tokenizer: Algo
        Fully configured ``Algo`` wrapper around a trained tokenizer.
    model: Module
        The trained PyTorch model to use for text generation.
    max_tokens: int, optional
        The maximum number of tokens to generate in case the end-of-sequence
        token is not predicted by the model first. Defaults to 256.
    width: int, optional
        Batch size to initialize buffers for the model. Defaults to 1.

    Note
    ----
    Child classes should also accept any number of additional, potentially
    unused keyword arguments so that they can be used as drop-ip replacements
    for each other.

    """

    def __init__(
            self,
            tokenizer: Algo,
            model: Module,
            max_tokens: int = 256,
            width: int = 1,
            **_: Any
    ) -> None:
        self.tokenizer = tokenizer
        self.model = model
        self.max_tokens = max_tokens
        self.width = width
        # Initialize model buffers with batch of maximum sequence length
        src = pt.randint(
            0,
            self.vocab,
            [width, self.context],
            device=model.device
        )
        mask = pt.zeros(
            1,
            self.context,
            dtype=model.dtype,
            device=model.device
        )
        self.model.eval()
        with pt.inference_mode():
            _ = self.model(src, None, mask, False)

    def __repr__(self) -> str:
        cls = self.__class__.__name__
        return f'{cls}(tokenizer, model, {self.max_tokens})'

    @property
    def vocab(self) -> int:
        """Actual vocabulary size of the `tokenizer` to use."""
        return self.tokenizer.get_vocab_size()

    @property
    def eos_id(self) -> int:
        """Integer ID of the end-of-sequence token."""
        return self.tokenizer.eos_id

    @property
    def context(self) -> int:
        """Maximum sequence length that the provided `model` can handle."""
        return self.model.context

    @cached_property
    def zero(self) -> Tensor:
        """Tensor of one 0 shaped to append to the unknown-token mask."""
        return pt.zeros(1, 1, dtype=self.model.dtype, device=self.model.device)

    def __call__(self, prompt: str) -> tuple[str, bool]:
        """Iteratively generate a text answer from the `model` responses.

        Parameters
        ----------
        prompt: str
            The input sequence that the model should continue.

        Returns
        -------
        answer: str
            The text answer generated by the model.
        terminated: bool
            Whether the model answer ended with an end-of-sequence token, the
            alternative being that the model answer exceeds the specified
            `max_tokens` length.

        """
        encoded = self.tokenizer.encode(prompt)
        encoded.truncate(self.context, direction='left')

        more = encoded.ids[-1] == self.eos_id  # Predict a non-EOS token first?

        src = pt.tensor(
            encoded.ids,
            device=self.model.device,
            dtype=pt.long
        ).unsqueeze(0)
        # Create a floating-point mask to not attend to unknown tokens.
        mask = pt.zeros_like(
            src,
            dtype=self.model.dtype,
            device=self.model.device
        ).where(
            src == self.tokenizer.unk_id,
            1.0
        ).log()

        self.model.eval()
        answer = self.predict(src, mask, more)
        return self.tokenizer.decode(answer), answer[-1] == self.eos_id

    @abstractmethod
    def predict(self, src: Tensor, mask: Tensor, more: bool) -> list[int]:
        """Subclasses implement how the model actually generates an answer.

        Parameters
        ----------
        src: Tensor
            PyTorch tensor with the `prompt` converted to integer token
            IDs of shape (1, `S`) with `S` being the number of tokens.
        mask: Tensor
            Additive attention mask of the same shape with `-inf` indicating
            positions that should *not* be attended to and 0 that they should.
        more: bool
            If ``True``, the input to the model ends with and end-of-sequence
            (EOS) token and, therefore, the model must predict at least one
            token that is *not* EOS. If ``False``, the model may predict EOS
            first, but must follow that up with at least one non-EOS token.

        Returns
        -------
        list
            Integer token IDs of the model response.

        """
        ...


class NextToken(Generator):
    """Abstract base class for strictly next-token-prediction generators.

    Parameters
    ----------
    tokenizer: Algo
        Fully configured ``Algo`` wrapper around a trained tokenizer.
    model: Module
        The trained PyTorch model to use for text generation.
    max_tokens: int, optional
        The maximum number of tokens to generate in case the end-of-sequence
        token is not predicted by the model first. Defaults to 256.

    Note
    ----
    Child classes should also accept any number of additional, potentially
    unused keyword arguments so that they can be used as drop-ip replacements
    for each other.

    """

    def __init__(
            self,
            tokenizer: Algo,
            model: Module,
            max_tokens: int = 256,
            **_: Any
    ) -> None:
        super().__init__(tokenizer, model, max_tokens, 1)

    def logits(self, src: Tensor, mask: Tensor, more: bool) -> Logits:
        """Call `model` to produce un-normalized probabilities over the vocab.

        Parameters
        ----------
        src: Tensor
            PyTorch tensor with the `prompt` converted to integer token
            IDs of shape (1, `S`) with `S` being the number of tokens.
        mask: Tensor
            Additive attention mask of the same shape with `-inf` indicating
            positions that should *not* be attended to and 0 that they should.
        more: bool
            If ``True``, the input to the model ends with and end-of-sequence
            (EOS) token and, therefore, the model must predict at least one
            token that is *not* EOS. If ``False``, the model may predict EOS
            first, but must follow that up with at least one non-EOS token.

        Returns
        -------
        logits: Tensor
            1-D PyTorch tensor with un-normalized probabilities over all
            permissible tokens in the vocabulary.
        offset: int
            Index of the token that the first entry in `logits` refers to.

        """
        with pt.inference_mode():
            out, *_ = self.model(src, None, mask, False)
        offset = self.eos_id + more  # Exclude EOS token if not permitted.
        return out[0, offset:self.vocab, -1].float(), offset

    def step(self, token: Tensor, src: Tensor, mask: Tensor) -> Tensors2T:
        """Concatenate a newly predicted token to the input sequence.

        Parameters
        ----------
        token: Tensor
            The ID of the token to append to `src` as an int64 tensor
            broadcastable to shape (1, 1).
        src: Tensor
            The input sequence of shape (1, `S`) with `S` being the number of
            tokens.
        mask: Tensor
            Additive attention mask of the same shape with `-inf` indicating
            positions that should *not* be attended to and 0 that they should.

        Returns
        -------
        src: Tensor
            The input sequence of maximum length `context` with the new
            `token` appended to it.
        mask: Tensor
            The input `mask` of maximum length `context` with a 0 appended.

        """
        mask = pt.cat([mask, self.zero], dim=-1)[:, -self.context:]
        src = pt.cat([src, token.view(1, 1)], dim=-1)[:, -self.context:]
        return src, mask

    def predict(self, src: Tensor, mask: Tensor, more: bool) -> list[int]:
        """Predict one permissible token at a time until end-of-sequence.

        Parameters
        ----------
        src: Tensor
            PyTorch tensor with the `prompt` converted to integer token
            IDs of shape (1, `S`) with `S` being the number of tokens.
        mask: Tensor
            Additive attention mask of the same shape with `-inf` indicating
            positions that should *not* be attended to and 0 that they should.
        more: bool
            If ``True``, the input to the model ends with and end-of-sequence
            (EOS) token and, therefore, the model must predict at least one
            token that is *not* EOS. If ``False``, the model may predict EOS
            first, but must follow that up with at least one non-EOS token.

        Returns
        -------
        list
            Integer token IDs of the model response.

        """
        answer = []
        # Predict the first token of the model answer.
        logits, offset = self.logits(src, mask, more)
        next_token = self.next_token_from_logits(logits) + offset
        answer.append(next_token.item())
        src, mask = self.step(next_token, src, mask)

        # If we restricted the first token to not be EOS, the next one can be.
        # If we did not, and it is, we need at least one more non-EOS token.
        more = False if more else next_token.item() == self.eos_id

        for _ in range(1, self.max_tokens):
            logits, offset = self.logits(src, mask, more)
            next_token = self.next_token_from_logits(logits) + offset
            answer.append(next_token.item())
            if next_token.item() == self.eos_id:  # Cannot be if more = True.
                break
            src, mask = self.step(next_token, src, mask)
            more = False  # From now on, EOS is acceptable in any case.

        return answer

    @abstractmethod
    def next_token_from_logits(self, logits: Tensor) -> Tensor:
        """Subclasses must implement how the next token is chosen from logits.

        Parameters
        ----------
        logits: Tensor
            1-D PyTorch tensor with un-normalized probabilities over all
            permissible tokens in the vocabulary.

        Returns
        -------
        Tensor
            Int64 scalar with the ID of the single next token chosen on the
            basis of the `logits`.

        """
        ...
